import chromadb
from sentence_transformers import SentenceTransformer, util
import pandas as pd
from tabulate import tabulate
import random
import csv
import os

# --- 1. CONFIGURACI√ìN GENERAL ---
CHROMA_DB_PATH = "./candidates_db"
COLLECTION_NAME = "cvu_candidatos"
ARCHIVO_VERDAD = "verdad_terreno.csv" # Tu archivo manual de etiquetas

# --- 2. PAR√ÅMETROS DE EXPERIMENTACI√ìN ---
SEMILLA = 42          # Fija esta semilla para tomar SIEMPRE los mismos CVs
TAMA√ëO_MUESTRA = 20   # Cantidad de CVs a evaluar
UMBRAL = 0.30         # Umbral de Similitud Coseno (de 0.0 a 1.0)

# --- 3. REQUISITOS FIJOS DEL PUESTO ---
TARGET_TITULO = "Software Engineer Developer"
TARGET_SKILLS = "Python, SQL, Leadership"
TARGET_EXP = 3

# --- 4. CASO DE USO A EVALUAR ---
# Define qu√© criterio de b√∫squeda auditar√°s
# 1=T√≠tulo, 2=Skills, 3=Experiencia, 4=T&S, 7=T&S&E (TODO)
CASO_A_EVALUAR = 7 


def conectar_db():
    """Conecta a la base de datos Chroma y carga el modelo de embeddings."""
    try:
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        col = client.get_collection(name=COLLECTION_NAME)
        model = SentenceTransformer('all-MiniLM-L6-v2')
        return col, model
    except Exception as e:
        print(f"‚ùå Error conectando a DB: {e}. ¬øEjecutaste 'ingesta_cvu.py'?")
        exit()

def obtener_muestra_controlada(col):
    """Obtiene un conjunto fijo de IDs usando la semilla."""
    todos_ids = col.get(include=[])['ids']
    total = len(todos_ids)
    
    if total < TAMA√ëO_MUESTRA:
        ids_muestra = todos_ids
        print(f"‚ö†Ô∏è Muestra ({TAMA√ëO_MUESTRA}) mayor que el total ({total}). Usando todos.")
    else:
        random.seed(SEMILLA)
        ids_muestra = random.sample(todos_ids, TAMA√ëO_MUESTRA)
        print(f"‚úÖ Muestra de {len(ids_muestra)} CVs seleccionada (Semilla {SEMILLA}).")

    datos = col.get(ids=ids_muestra, include=['metadatas'])
    metadatas = {id_: meta for id_, meta in zip(datos['ids'], datos['metadatas'])}
    return ids_muestra, metadatas

def evaluar_candidato(meta, model):
    """
    L√≥gica de evaluaci√≥n blindada con Debugging.
    """
    nombre = meta.get('candidate_name', 'Unknown')
    
    # --- 1. EVALUACI√ìN T√çTULO ---
    emb_t1 = model.encode(TARGET_TITULO, convert_to_tensor=True)
    emb_t2 = model.encode(meta.get('titles', ''), convert_to_tensor=True)
    score_t = util.cos_sim(emb_t1, emb_t2).item()
    val_t = 1 if score_t >= UMBRAL else 0
    
    # --- 2. EVALUACI√ìN SKILLS ---
    emb_s1 = model.encode(TARGET_SKILLS, convert_to_tensor=True)
    emb_s2 = model.encode(meta.get('skills', ''), convert_to_tensor=True)
    score_s = util.cos_sim(emb_s1, emb_s2).item()
    val_s = 1 if score_s >= UMBRAL else 0
    
    # --- 3. EVALUACI√ìN EXPERIENCIA (Blindaje de Tipos) ---
    raw_exp = meta.get('years_experience', 0)
    try:
        # Convertimos a float primero por si viene como "3.5" y luego a int
        exp_num = int(float(raw_exp))
    except:
        exp_num = 0
        
    val_e = 1 if exp_num >= TARGET_EXP else 0

    # --- DEBUGGING (Para que veas por qu√© da 0) ---
    # Esto imprimir√° en consola los valores internos
    # print(f"üîç DEBUG {nombre[:15]}: T({score_t:.2f})={val_t} | S({score_s:.2f})={val_s} | Exp({exp_num})={val_e} --> CASO {CASO_A_EVALUAR}")

    # --- L√ìGICA DE SELECCI√ìN DE CASO ---
    resultado_final = 0
    
    if CASO_A_EVALUAR == 1: # Solo T√≠tulo
        resultado_final = val_t
    elif CASO_A_EVALUAR == 2: # Solo Skills
        resultado_final = val_s
    elif CASO_A_EVALUAR == 3: # Solo Experiencia
        resultado_final = val_e
    elif CASO_A_EVALUAR == 4: # T√≠tulo AND Skills
        resultado_final = 1 if (val_t and val_s) else 0
    elif CASO_A_EVALUAR == 5: # T√≠tulo AND Exp
        resultado_final = 1 if (val_t and val_e) else 0
    elif CASO_A_EVALUAR == 6: # Skills AND Exp
        resultado_final = 1 if (val_s and val_e) else 0
    elif CASO_A_EVALUAR == 7: # TODO
        resultado_final = 1 if (val_t and val_s and val_e) else 0
        
    return resultado_final

def cargar_verdad_terreno(ids_muestra):
    """Carga tu evaluaci√≥n manual desde el CSV y valida los IDs."""
    verdad = {}
    ids_en_csv = set()
    try:
        with open(ARCHIVO_VERDAD, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            headers = next(reader) # Leer cabecera

            # Asegurar que el CSV tenga las dos columnas esperadas
            if len(headers) < 2 or 'ID_Archivo' not in headers[0] or 'Etiqueta_Humana' not in headers[1]:
                print(f"‚ùå Error en cabecera. Aseg√∫rate de que las columnas son: ID_Archivo, Etiqueta_Humana")
                exit()
                
            for row in reader:
                if len(row) >= 2:
                    doc_id = row[0].strip()
                    try:
                        etiqueta = int(row[1].strip())
                        if doc_id in ids_muestra:
                            verdad[doc_id] = etiqueta
                            ids_en_csv.add(doc_id)
                    except ValueError:
                        print(f"‚ö†Ô∏è Ignorando fila de {doc_id}: La etiqueta humana debe ser 1 o 0.")
        
        if len(ids_en_csv) != len(ids_muestra):
            print(f"‚ö†Ô∏è Advertencia: El CSV solo tiene {len(ids_en_csv)} etiquetas, se esperaban {len(ids_muestra)}.")
            
        return verdad
    except FileNotFoundError:
        return None

def generar_reporte_muestra(ids_muestra, metadatas):
    """Genera el archivo CSV inicial para que el humano etiquete."""
    print("----------------------------------------------------------")
    print(f" PASO 1: GENERAR ARCHIVO BASE PARA ETIQUETADO MANUAL")
    print("----------------------------------------------------------")
    
    lista_datos = []
    for doc_id in ids_muestra:
        m = metadatas[doc_id]
        nombre = m.get('candidate_name', 'Unknown')
        lista_datos.append([
            doc_id, 
            0, # Columna Etiqueta Humana: DEBES rellenar con 1 (Cumple) o 0 (No Cumple)
            nombre,
            m.get('titles', '')[:40],
            m.get('skills', '')[:40],
            m.get('years_experience', 0)
        ])
    
    # Crear el CSV de salida
    headers = ["ID_Archivo", "Etiqueta_Humana", "Candidato", "Titulo_Detectado", "Skills_Detectadas", "Exp_Detectada"]
    try:
        with open(ARCHIVO_VERDAD, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            writer.writerows(lista_datos)
        
        print(f"üíæ Archivo '{ARCHIVO_VERDAD}' creado. Contiene {len(ids_muestra)} CVs.")
        print(f"üëÄ TAREA: Abre el CSV, eval√∫a la columna 'Etiqueta_Humana' (1 o 0) y vuelve a ejecutar el script.")
        print("----------------------------------------------------------")
        return False
        
    except Exception as e:
        print(f"‚ùå Error al escribir el CSV: {e}")
        return False

def ejecutar_auditoria_precision():
    col, model = conectar_db()
    
    # Obtener IDs y metadatos de la muestra fija
    ids_muestra, metadatas = obtener_muestra_controlada(col)
    
    # Intentar cargar la verdad terreno (etiquetas humanas)
    verdad_humana = cargar_verdad_terreno(ids_muestra)

    if verdad_humana is None or not verdad_humana:
        # Si el CSV no existe o est√° vac√≠o, genera el archivo base y termina
        generar_reporte_muestra(ids_muestra, metadatas)
        return
    
    # --- PROCESO DE AUDITOR√çA (Solo si hay etiquetas humanas) ---
    print("\n----------------------------------------------------------")
    print(f" PASO 2: COMPARANDO M√ÅQUINA vs HUMANO (Caso {CASO_A_EVALUAR})")
    print("----------------------------------------------------------")

    # Contadores Matriz Confusi√≥n
    tp = 0 # Humano: 1, M√°quina: 1 (Verdadero Positivo - Acierto)
    tn = 0 # Humano: 0, M√°quina: 0 (Verdadero Negativo - Acierto)
    fp = 0 # Humano: 0, M√°quina: 1 (Falso Positivo - Error: M√°quina es muy optimista)
    fn = 0 # Humano: 1, M√°quina: 0 (Falso Negativo - Error: M√°quina es muy estricta/fallo de extracci√≥n)

    detalles_error = []

    for doc_id in verdad_humana.keys():
        decision_maquina = evaluar_candidato(metadatas[doc_id], model)
        decision_humana = verdad_humana[doc_id]
        
        nombre = metadatas[doc_id].get('candidate_name', 'Unknown')

        if decision_humana == 1 and decision_maquina == 1: tp += 1
        elif decision_humana == 0 and decision_maquina == 0: tn += 1
        elif decision_humana == 0 and decision_maquina == 1:
            fp += 1
            detalles_error.append([nombre, "H:NO, M:S√ç", "Falso Positivo (M√°quina optimista)"])
        elif decision_humana == 1 and decision_maquina == 0:
            fn += 1
            detalles_error.append([nombre, "H:S√ç, M:NO", "Falso Negativo (M√°quina estricta)"])

    # --- REPORTE DE MATRIZ DE CONFUSI√ìN ---
    print("\n" + "="*60)
    print("üìä MATRIZ DE CONFUSI√ìN (PRECISI√ìN AI vs. Criterio Humano)")
    print("="*60)
    
    matriz = [
        ["", "M√°quina: S√ç", "M√°quina: NO"],
        ["Humano: S√ç", f"‚úÖ {tp} (VP)", f"‚ùå {fn} (FN)"],
        ["Humano: NO", f"‚ö†Ô∏è {fp} (FP)", f"‚úÖ {tn} (VN)"]
    ]
    print(tabulate(matriz, tablefmt="grid"))
    
    total_evaluado = tp + tn + fp + fn
    if total_evaluado > 0:
        accuracy = (tp + tn) / total_evaluado * 100
        print(f"\nüéØ EXACTITUD (ACCURACY) GLOBAL: {accuracy:.2f}%")
        
        # M√©tricas adicionales (Opcional, pero √∫til)
        precision_calc = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall_calc = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        print(f"   - Precisi√≥n (Solo aciertos cuando dijo S√ç): {precision_calc:.2f}")
        print(f"   - Recall (Capacidad para encontrar S√çes): {recall_calc:.2f}")
    
    if detalles_error:
        print("\nüîç DETALLE DE ERRORES:")
        print(tabulate(detalles_error, headers=["Candidato", "Cruce", "Tipo Error"], tablefmt="simple"))

if __name__ == "__main__":
    ejecutar_auditoria_precision()